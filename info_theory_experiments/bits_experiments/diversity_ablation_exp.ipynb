{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def setup_project_root(start_path='.'):\n",
    "    \"\"\"Find the project root, set it as the current working directory, and add it to sys.path.\"\"\"\n",
    "    current_path = os.path.abspath(start_path)\n",
    "    while True:\n",
    "        if '.git' in os.listdir(current_path):\n",
    "            project_root = current_path\n",
    "            break\n",
    "        parent_path = os.path.dirname(current_path)\n",
    "        if parent_path == current_path:  # We've reached the root directory\n",
    "            raise Exception(\"Could not find project root (.git directory not found)\")\n",
    "        current_path = parent_path\n",
    "    \n",
    "    # Change the current working directory to the project root\n",
    "    os.chdir(project_root)\n",
    "    print(f\"Current working directory set to: {os.getcwd()}\")\n",
    "\n",
    "    # Add project root to sys.path if it's not already there\n",
    "    if project_root not in sys.path:\n",
    "        sys.path.insert(0, project_root)\n",
    "        print(f\"Added {project_root} to sys.path\")\n",
    "\n",
    "# sets the current working directory to the project root\n",
    "setup_project_root()\n",
    "\n",
    "# Don't cache imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from info_theory_experiments.custom_datasets import BitStringDataset\n",
    "from info_theory_experiments.models import SupervenientFeatureNetwork, SkipConnectionSupervenientFeatureNetwork\n",
    "import torch\n",
    "from info_theory_experiments.trainers import train_feature_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment to show that without the diversity loss term model learns same emergent features\n",
    "This will work by training some different feature networks, freezing them, running them in eval mode, and showing they all learn the same bit\n",
    "\n",
    "When we verify we will use relatively weaker critics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for seed in range(5):\n",
    "    bits_config_train = {\n",
    "            \"gamma_parity\": 0.99,\n",
    "            \"gamma_extra\": 0.99,\n",
    "            \"dataset_length\": 1000000,\n",
    "            \"torch_seed\": seed,\n",
    "            \"dataset_type\": \"bits\",\n",
    "            \"num_atoms\": 6,\n",
    "            \"batch_size\": 1000,\n",
    "            \"train_mode\": True,\n",
    "            \"train_model_B\": False,\n",
    "            \"adjust_Psi\": False,\n",
    "            \"clip\": 5,\n",
    "            \"feature_size\": 1,\n",
    "            \"epochs\": 5,\n",
    "            \"start_updating_f_after\": 1000,\n",
    "            \"update_f_every_N_steps\": 5,\n",
    "            \"minimize_neg_terms_until\": 0,\n",
    "            \"downward_critics_config\": {\n",
    "                \"hidden_sizes_v_critic\": [512, 512, 512, 256],\n",
    "                \"hidden_sizes_xi_critic\": [512, 512, 512, 256],\n",
    "                \"critic_output_size\": 32,\n",
    "                \"lr\": 1e-3,\n",
    "                \"bias\": True,\n",
    "                \"weight_decay\": 0,\n",
    "            },\n",
    "            \n",
    "            \"decoupled_critic_config\": {\n",
    "                \"hidden_sizes_encoder_1\": [512, 512, 512],\n",
    "                \"hidden_sizes_encoder_2\": [512, 512, 512],\n",
    "                \"critic_output_size\": 32,\n",
    "                \"lr\": 1e-3,\n",
    "                \"bias\": True,\n",
    "                \"weight_decay\": 0,\n",
    "            },\n",
    "            \"feature_network_config\": {\n",
    "                \"hidden_sizes\": [256, 256],\n",
    "                \"lr\": 1e-4,\n",
    "                \"bias\": True,\n",
    "                \"weight_decay\": 0.00001,\n",
    "            }\n",
    "    }\n",
    "\n",
    "    dataset = BitStringDataset(\n",
    "        gamma_extra=bits_config_train[\"gamma_extra\"],\n",
    "        gamma_parity=bits_config_train[\"gamma_parity\"],\n",
    "        length=bits_config_train[\"dataset_length\"],\n",
    "    )\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=bits_config_train[\"batch_size\"], shuffle=False\n",
    "    )\n",
    "\n",
    "    skip_model = SkipConnectionSupervenientFeatureNetwork(\n",
    "        num_atoms=bits_config_train['num_atoms'],\n",
    "        feature_size=bits_config_train['feature_size'],\n",
    "        hidden_sizes=bits_config_train['feature_network_config']['hidden_sizes'],\n",
    "        include_bias=bits_config_train['feature_network_config']['bias'],\n",
    "    ).to(device)\n",
    "\n",
    "    project_name_train = \"NEURIPS-diversity-ablation-training\"\n",
    "\n",
    "    skip_model = train_feature_network(\n",
    "            config=bits_config_train,\n",
    "            trainloader=trainloader,\n",
    "            feature_network_training=skip_model,\n",
    "            project_name=project_name_train,\n",
    "            model_dir_prefix=None\n",
    "    )\n",
    "\n",
    "    bits_config_test = {\n",
    "            \"gamma_parity\": 0.99,\n",
    "            \"gamma_extra\": 0.99,\n",
    "            \"dataset_length\": 1000000,\n",
    "            \"torch_seed\": seed,\n",
    "            \"dataset_type\": \"bits\",\n",
    "            \"num_atoms\": 6,\n",
    "            \"batch_size\": 1000,\n",
    "            \"train_mode\": False,\n",
    "            \"train_model_B\": False,\n",
    "            \"adjust_Psi\": False,\n",
    "            \"clip\": 5,\n",
    "            \"feature_size\": 1,\n",
    "            \"epochs\": 2,\n",
    "            \"start_updating_f_after\": 1000,\n",
    "            \"update_f_every_N_steps\": 5,\n",
    "            \"minimize_neg_terms_until\": 0,\n",
    "            \"downward_critics_config\": {\n",
    "                \"hidden_sizes_v_critic\": [256, 256, 256],\n",
    "                \"hidden_sizes_xi_critic\": [256, 256, 256],\n",
    "                \"critic_output_size\": 32,\n",
    "                \"lr\": 1e-3,\n",
    "                \"bias\": True,\n",
    "                \"weight_decay\": 0,\n",
    "            },\n",
    "            \n",
    "            \"decoupled_critic_config\": {\n",
    "                \"hidden_sizes_encoder_1\": [256, 256],\n",
    "                \"hidden_sizes_encoder_2\": [256, 256],\n",
    "                \"critic_output_size\": 32,\n",
    "                \"lr\": 1e-3,\n",
    "                \"bias\": True,\n",
    "                \"weight_decay\": 0,\n",
    "            },\n",
    "            \"feature_network_config\": {\n",
    "                \"hidden_sizes\": [256, 256],\n",
    "                \"lr\": 1e-4,\n",
    "                \"bias\": True,\n",
    "                \"weight_decay\": 0.00001,\n",
    "            }\n",
    "    }\n",
    "\n",
    "    project_name_test = \"NEURIPS-diversity-ablation-test\"\n",
    "\n",
    "    skil_model = train_feature_network(\n",
    "            config=bits_config_test,\n",
    "            trainloader=trainloader,\n",
    "            feature_network_training=skip_model,\n",
    "            project_name=project_name_test,\n",
    "            model_dir_prefix=None\n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
