{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "import tqdm\n",
    "from info_theory_experiments.models import (SupervenientFeatureNetwork,\n",
    "                    CLUB,\n",
    "                    DecoupledSmileMIEstimator,\n",
    "                    DownwardSmileMIEstimator,\n",
    "                    SkipConnectionSupervenientFeatureNetwork\n",
    "                    )\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from info_theory_experiments.custom_datasets import ECoGDataset\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this experimetn we train an emergent feture network on a dataset of ECoG data\n",
    "\n",
    "some code like `train_feature_network` should be replaced by the more general trainer found in `trainers.py` for simplicity, which will be done during a house-keeping codebase update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_feature_network(config, trainloader, feature_network):\n",
    "\n",
    "    wandb.init(project=\"ecog-dataset-neurips\", config=config)\n",
    "    # init weights to zero of the feature network\n",
    "\n",
    "    decoupled_MI_estimator = DecoupledSmileMIEstimator(\n",
    "        feature_size=config['feature_size'],\n",
    "        critic_output_size=config['critic_output_size'],\n",
    "        hidden_sizes_1=config['decoupled_critic_hidden_sizes_1'],\n",
    "        hidden_sizes_2=config['decoupled_critic_hidden_sizes_2'],\n",
    "        clip=config['clip'],\n",
    "        include_bias=config['bias']\n",
    "        ).to(device)\n",
    "    downward_MI_estimators = [\n",
    "        DownwardSmileMIEstimator(\n",
    "            feature_size=config['feature_size'],\n",
    "            critic_output_size=config['critic_output_size'],\n",
    "            hidden_sizes_v_critic=config['downward_hidden_sizes_v_critic'],\n",
    "            hidden_sizes_xi_critic=config['downward_hidden_sizes_xi_critic'],\n",
    "            clip=config['clip'],\n",
    "            include_bias=config['bias']\n",
    "            ).to(device) \n",
    "        for _ in range(config['num_atoms'])\n",
    "    ]\n",
    "    \n",
    "\n",
    "    feature_optimizer = torch.optim.Adam(\n",
    "        feature_network.parameters(),\n",
    "        lr=config[\"feature_lr\"],\n",
    "        weight_decay=config[\"weight_decay\"]\n",
    "    )\n",
    "    decoupled_optimizer = torch.optim.Adam(\n",
    "        decoupled_MI_estimator.parameters(),\n",
    "        lr=config[\"decoupled_critic_lr\"],\n",
    "        weight_decay=config[\"weight_decay\"]\n",
    "    )\n",
    "    downward_optims = [\n",
    "        torch.optim.Adam(\n",
    "            dc.parameters(),\n",
    "            lr=config[\"downward_lr\"],\n",
    "            weight_decay=config[\"weight_decay\"]\n",
    "        ) \n",
    "        for dc in downward_MI_estimators\n",
    "    ]\n",
    "\n",
    "    wandb.watch(feature_network, log='all')\n",
    "    wandb.watch(decoupled_MI_estimator, log=\"all\")\n",
    "    for dc in downward_MI_estimators:\n",
    "        wandb.watch(dc, log='all')\n",
    "\n",
    "    ##\n",
    "    ## TRAIN FEATURE NETWORK\n",
    "    ##\n",
    "\n",
    "    epochs = config['epochs']\n",
    "\n",
    "    step = 0\n",
    "\n",
    "    for _ in tqdm.tqdm(range(epochs), desc='Training'):\n",
    "        for batch_num, batch in enumerate(trainloader):\n",
    "            x0 = batch[:, 0].to(device).float()\n",
    "            x1 = batch[:, 1].to(device).float()\n",
    "            v0 = feature_network(x0).detach()\n",
    "            v1 = feature_network(x1).detach()\n",
    "\n",
    "            # update decoupled critic\n",
    "            decoupled_optimizer.zero_grad()\n",
    "            decoupled_MI = decoupled_MI_estimator(v0, v1)\n",
    "            decoupled_loss = -decoupled_MI\n",
    "            decoupled_loss.backward(retain_graph=True)\n",
    "            decoupled_optimizer.step()\n",
    "\n",
    "            # update each downward critic \n",
    "            for i in range(config['num_atoms']):\n",
    "                downward_optims[i].zero_grad()\n",
    "                channel_i = x0[:, i].unsqueeze(1).detach()\n",
    "                downward_MI_i = downward_MI_estimators[i](v1, channel_i)\n",
    "                downward_loss = - downward_MI_i\n",
    "                downward_loss.backward(retain_graph=True)\n",
    "                downward_optims[i].step()\n",
    "                wandb.log({\n",
    "                    f\"downward_MI_{i}\": downward_MI_i   \n",
    "                }, step=step)\n",
    "\n",
    "            # update feature network   \n",
    "            feature_optimizer.zero_grad()\n",
    "            channel_MIs = []\n",
    "\n",
    "            MIs = []\n",
    "            v0 = feature_network(x0)\n",
    "            v1 = feature_network(x1)\n",
    "\n",
    "            for i in range(config['num_atoms']):\n",
    "                channel_i = x0[:, i].unsqueeze(1)\n",
    "                channel_i_MI = downward_MI_estimators[i](v1, channel_i)\n",
    "                channel_MIs.append(channel_i_MI)\n",
    "                MIs.append(channel_i_MI)\n",
    "\n",
    "            sum_downward_MI = sum(channel_MIs)\n",
    "\n",
    "            decoupled_MI1 = decoupled_MI_estimator(v0, v1)\n",
    "\n",
    "            clipped_min_MIs = max(0, min(MIs))\n",
    "\n",
    "            Psi = decoupled_MI1 - sum_downward_MI + (config['num_atoms'] - 1) * clipped_min_MIs\n",
    "\n",
    "            # NOTE an experiment\n",
    "            feature_loss = sum_downward_MI \n",
    "\n",
    "\n",
    "            if config['start_updating_f_after'] < step:\n",
    "                if batch_num % config['update_f_every_N_steps'] == 0:\n",
    "                    feature_loss.backward(retain_graph=True)\n",
    "                    feature_optimizer.step()\n",
    "\n",
    "            wandb.log({\n",
    "                \"decoupled_MI\": decoupled_MI1,\n",
    "                \"sum_downward_MI\": sum_downward_MI,\n",
    "                \"Psi\": Psi,\n",
    "            }, step=step)\n",
    "\n",
    "\n",
    "            step += 1\n",
    "        \n",
    "    torch.save(feature_network.state_dict(), f\"models/ecog_feature_network_{wandb.run.name}.pth\")\n",
    "    \n",
    "    return feature_network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\": 1000,\n",
    "    \"num_atoms\": 64,\n",
    "    \"feature_size\": 3,\n",
    "    \"clip\": 5,\n",
    "    \"epochs\": 50,\n",
    "    \"critic_output_size\": 32,\n",
    "    \"downward_hidden_sizes_v_critic\": [512, 512, 512, 512],\n",
    "    \"downward_hidden_sizes_xi_critic\": [512, 512, 512],\n",
    "    \"feature_hidden_sizes\": [256, 256, 256, 256, 256],\n",
    "    \"decoupled_critic_hidden_sizes_1\": [512, 512, 512],\n",
    "    \"decoupled_critic_hidden_sizes_2\": [512, 512, 512],\n",
    "    \"feature_lr\": 1e-4,\n",
    "    \"decoupled_critic_lr\": 1e-3,\n",
    "    \"downward_lr\": 1e-3,    \n",
    "    \"bias\": True,\n",
    "    \"update_f_every_N_steps\": 5,\n",
    "    \"weight_decay\": 0,\n",
    "    \"start_updating_f_after\": 300,\n",
    "    \"add_spec_norm_downward\": False,\n",
    "    \"add_spec_norm_decoupled\": False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ECoGDataset()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_newtork = SkipConnectionSupervenientFeatureNetwork(\n",
    "    num_atoms=config['num_atoms'],\n",
    "    feature_size=config['feature_size'],\n",
    "    hidden_sizes=config['feature_hidden_sizes'],\n",
    "    include_bias=config['bias']\n",
    ").to(device)\n",
    "\n",
    "feature_network = train_feature_network(config, train_loader, feature_newtork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Psi given a frozen feature network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_true_Psi(feature_network, feature_config, run_id=None):\n",
    "\n",
    "    config = {\n",
    "        \"batch_size\": 600,\n",
    "        \"num_atoms\": 64,\n",
    "        \"feature_size\": feature_config['feature_size'],\n",
    "        \"clip\": 5,\n",
    "        \"critic_output_size\": 16,\n",
    "        \"downward_hidden_sizes_v_critic\": [1028, 1028, 512, 64],\n",
    "        \"downward_hidden_sizes_xi_critic\": [512, 512, 512, 64],\n",
    "        \"decoupled_critic_hidden_sizes_1\": [1028, 1028, 512],\n",
    "        \"decoupled_critic_hidden_sizes_2\": [1028, 1028, 512],\n",
    "        \"decoupled_critic_lr\": 1e-4,\n",
    "        \"downward_lr\": 1e-4,\n",
    "        \"bias\": True,\n",
    "        \"weight_decay\": 1e-6,\n",
    "        \"original_run_id\": run_id,\n",
    "        \"add_spec_norm_downward\": False,\n",
    "        \"add_spec_norm_decoupled\": False\n",
    "    }\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "\n",
    "    wandb.init(project=\"Finding-true-Psi-for-f\", config=config, id=run_id)\n",
    "\n",
    "    decoupled_critic = DecoupledSmileMIEstimator(\n",
    "        feature_size=config['feature_size'],\n",
    "        critic_output_size=config['critic_output_size'],\n",
    "        hidden_sizes_1=config['decoupled_critic_hidden_sizes_1'],\n",
    "        hidden_sizes_2=config['decoupled_critic_hidden_sizes_2'],\n",
    "        clip=config['clip'],\n",
    "        include_bias=config['bias'],\n",
    "        add_spec_norm=config['add_spec_norm_decoupled']\n",
    "        ).to(device)\n",
    "\n",
    "    downward_critics = [\n",
    "        DownwardSmileMIEstimator(\n",
    "            feature_size=config['feature_size'],\n",
    "            critic_output_size=config['critic_output_size'],\n",
    "            hidden_sizes_v_critic=config['downward_hidden_sizes_v_critic'],\n",
    "            hidden_sizes_xi_critic=config['downward_hidden_sizes_xi_critic'],\n",
    "            clip=config['clip'],\n",
    "            include_bias=config['bias'],\n",
    "            add_spec_norm=config['add_spec_norm_downward']\n",
    "            ).to(device) \n",
    "        for _ in range(config['num_atoms'])\n",
    "    ]\n",
    "\n",
    "    downward_optims = [\n",
    "        torch.optim.Adam(\n",
    "            dc.parameters(),\n",
    "            lr=config[\"downward_lr\"],\n",
    "            weight_decay=config[\"weight_decay\"]\n",
    "        ) \n",
    "        for dc in downward_critics\n",
    "    ]\n",
    "\n",
    "    decoupled_optimizer = torch.optim.Adam(\n",
    "        decoupled_critic.parameters(),\n",
    "        lr=config[\"decoupled_critic_lr\"],\n",
    "        weight_decay=config[\"weight_decay\"]\n",
    "    )\n",
    "\n",
    "    wandb.watch(decoupled_critic, log=\"all\")\n",
    "    for dc in downward_critics:\n",
    "        wandb.watch(dc, log='all')\n",
    "\n",
    "    epochs = 5\n",
    "\n",
    "    for _ in tqdm.tqdm(range(epochs), desc='Training'):\n",
    "        for _, batch in enumerate(trainloader):\n",
    "            x0 = batch[:, 0].to(device).float()\n",
    "            x1 = batch[:, 1].to(device).float()\n",
    "\n",
    "            # update decoupled critic\n",
    "\n",
    "            v0 = feature_network(x0)\n",
    "            v1 = feature_network(x1) \n",
    "\n",
    "            decoupled_optimizer.zero_grad()\n",
    "            decoupled_MI = decoupled_critic(v0, v1)\n",
    "            decoupled_loss = -decoupled_MI\n",
    "            decoupled_loss.backward(retain_graph=True)\n",
    "            decoupled_optimizer.step()\n",
    "\n",
    "\n",
    "            # update each downward critic \n",
    "\n",
    "            MIs = []\n",
    "\n",
    "            for i in range(config['num_atoms']):\n",
    "                downward_optims[i].zero_grad()\n",
    "                channel_i = x0[:, i].unsqueeze(1)\n",
    "                downward_MI_i = downward_critics[i](v1, channel_i)\n",
    "                # add spectral norm to the loss\n",
    "                downward_loss = - downward_MI_i\n",
    "                downward_loss.backward(retain_graph=True)\n",
    "                downward_optims[i].step()\n",
    "                wandb.log({\n",
    "                    f\"downward_MI_{i}\": downward_MI_i   \n",
    "                })\n",
    "                MIs.append(downward_MI_i)\n",
    "\n",
    "            # update feature network   \n",
    "\n",
    "            min_MI = min(MIs)\n",
    "            clipped_min_MIs = max(0, min_MI)\n",
    "\n",
    "            sum_downward_MI = 0\n",
    "\n",
    "            for i in range(config['num_atoms']):\n",
    "                channel_i = x0[:, i].unsqueeze(1)\n",
    "                sum_downward_MI += downward_critics[i](v1, channel_i)\n",
    "\n",
    "            decoupled_MI1 = decoupled_critic(v0, v1)\n",
    "\n",
    "            Psi = decoupled_MI1 - sum_downward_MI + (config['num_atoms'] - 1) * clipped_min_MIs\n",
    "\n",
    "            wandb.log({\n",
    "                \"decoupled_MI\": decoupled_MI1,\n",
    "                \"sum_downward_MI\": sum_downward_MI,\n",
    "                \"Psi\": Psi,\n",
    "            })\n",
    "        \n",
    "    return Psi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Psi = find_true_Psi(feature_network, feature_config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
